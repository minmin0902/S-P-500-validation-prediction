{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Anaconda3\\envs\\myvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 데이터 로드\n",
    "reddit_news = pd.read_csv('data/Combined_News_DJIA.csv')\n",
    "\n",
    "\n",
    "# 날짜 형식 변환 및 인덱스 설정\n",
    "reddit_news['Date'] = pd.to_datetime(reddit_news['Date'])\n",
    "reddit_news.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "# 뉴스 데이터를 하나의 문자열로 결합\n",
    "reddit_news['News'] = reddit_news[[col for col in reddit_news.columns if  col == 'Top1' or 'Top2'or 'Top3' or 'Top4' or'Top5']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)\n",
    "reddit_news['News'] = reddit_news['News'].replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"', '', regex=True)\n",
    "\n",
    "# 피처 및 타겟 설정\n",
    "X = reddit_news['News'].values\n",
    "y = reddit_news['Label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "bert_model = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = NewsDataset(X_train, y_train)\n",
    "test_dataset = NewsDataset(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [01:08<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 68.93, 'train_samples_per_second': 69.244, 'train_steps_per_second': 4.352, 'train_loss': 0.2504034423828125, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.2504034423828125, metrics={'train_runtime': 68.93, 'train_samples_per_second': 69.244, 'train_steps_per_second': 4.352, 'total_flos': 0.0, 'train_loss': 0.2504034423828125, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.regressor = nn.Linear(bert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()  # Add Sigmoid layer\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        logits = self.regressor(pooled_output)\n",
    "        logits = self.sigmoid(logits)  # Apply Sigmoid activation\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MSELoss()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "        \n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "# 회귀 모델 인스턴스 생성\n",
    "regression_model = RegressionModel(bert_model)\n",
    "\n",
    "# 트레이너 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    eval_steps=500,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=regression_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.24645309150218964, 'eval_runtime': 1.957, 'eval_samples_per_second': 203.372, 'eval_steps_per_second': 12.775, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2464530857195152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,accuracy_score,precision_score,recall_score,f1_score\n",
    "import numpy as np\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# 예측\n",
    "predictions = trainer.predict(test_dataset).predictions\n",
    "predictions = predictions.flatten()\n",
    "\n",
    "\n",
    "# MSE 계산\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.78%\n",
      "Precision: 56.93%\n",
      "Recall: 99.56%\n",
      "F1 Score: 72.44%\n"
     ]
    }
   ],
   "source": [
    "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
    "\n",
    "\n",
    "# 평가 지표 계산\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "precision = precision_score(y_test, binary_predictions, pos_label=1)\n",
    "recall = recall_score(y_test, binary_predictions, pos_label=1)\n",
    "f1 = f1_score(y_test, binary_predictions, pos_label=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision * 100:.2f}%')\n",
    "print(f'Recall: {recall * 100:.2f}%')\n",
    "print(f'F1 Score: {f1 * 100:.2f}%')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
